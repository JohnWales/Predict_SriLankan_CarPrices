{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a688db",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f859b548",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# # Useful for plotting graphs\n",
    "import matplotlib.pyplot as plt\n",
    "from seaborn import scatterplot\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# # Intent for data exploration\n",
    "from pandas.plotting import scatter_matrix\n",
    "from seaborn import scatterplot\n",
    "\n",
    "# # Important in order to create a test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# # For feature engineering\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from joblib import dump\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import validation_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "from seaborn import lmplot, stripplot\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70e0774",
   "metadata": {},
   "source": [
    "# Read in and Check the Data\n",
    "#### I also shuffle the dataset which will ensure randomisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e86e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset_vehicles.csv')\n",
    "\n",
    "df = df.sample(frac=1, random_state=2)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284f62c6",
   "metadata": {},
   "source": [
    "#### Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3adb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab2de32",
   "metadata": {},
   "source": [
    "#### Checking all the columns in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3cb623",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f267761",
   "metadata": {},
   "source": [
    "#### This checks to see how many items are in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20fc803",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdb8c2c",
   "metadata": {},
   "source": [
    "#### Illustrating the dimensions of the dataframe. 18938 rows, 19 columns. Shape can be used to determine whether we should use K fold or Holdout. In this case it would be Holdout as it is a large dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf2a316",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a735471f",
   "metadata": {},
   "source": [
    "#### Showcasing the datatypes. Notice how there is only 1 integer, the rest are strings. It is clear that I need to convert some objects to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8898bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7c5ff2",
   "metadata": {},
   "source": [
    "#### Visual representation of the dataframe. Initial reaction indicates that the dataframe needs to be cleaned. Only showing a few rows to minimise leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba411605",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a02ea9",
   "metadata": {},
   "source": [
    "#### Taking a copy of the df incase I make a big error, I still have the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0ce32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996a63b1",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "#### This includes converting objects to integers and removing outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28277ffe",
   "metadata": {},
   "source": [
    "#### Removing \"Rs\" from the price. Removing the comma so it can be converted to an integer. Converting to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3730150",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Price'] = df1['Price'].str.replace('Rs', '')\n",
    "df1[\"Price\"] = df1[\"Price\"].str.replace(',', '')\n",
    "df1[\"Price\"] = df1[\"Price\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75f48a93",
   "metadata": {},
   "source": [
    "#### Renaming Mileage to Mileage_km so I can extract km in the data to make the values an integer. The reason for renaiming is, so when the value is gone, future readers will understand from the title what it is measured in. The user could eaily think it is miles instead of km as some countries use different measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82750ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.rename(columns={'Mileage':'Mileage_km'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efbc195",
   "metadata": {},
   "source": [
    "#### Now it is time to remove \"km: in the values. Removing the comma so it can be converted to an integer. Converting to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0912ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"Mileage_km\"] = df1[\"Mileage_km\"].str.replace('km', '')\n",
    "df1[\"Mileage_km\"] = df1[\"Mileage_km\"].str.replace(',', '')\n",
    "df1[\"Mileage_km\"] = df1[\"Mileage_km\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ff439b",
   "metadata": {},
   "source": [
    "#### Renaming Capacity to Capacity so I can extract cc in the data to make it an integer. The reason for renaiming is so when the value is gone future readers will understand from the title what it is measured in. In this case it is measured in cc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75336316",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.rename(columns={'Capacity':'Capacity_cc'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a2bfe6",
   "metadata": {},
   "source": [
    "#### Removing \"cc\" from the capacity column. Removing the comma so it can be converted to an integer. Converting to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ad28f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"Capacity_cc\"] = df1[\"Capacity_cc\"].str.replace('cc', '')\n",
    "df1[\"Capacity_cc\"] = df1[\"Capacity_cc\"].str.replace(',', '')\n",
    "df1[\"Capacity_cc\"] = df1[\"Capacity_cc\"].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e05d28",
   "metadata": {},
   "source": [
    "#### Demonstrating that I have converted the objects/strings that should be integers, into integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0636f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37563e5",
   "metadata": {},
   "source": [
    "#### Checking to see if my conversions have worked on the data visually. Only showing head to minimise leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4064b175",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7756c663",
   "metadata": {},
   "source": [
    "#### Check for NaN in the case of the target values. It is False so nothing further needs to be done. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b925cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1[\"Price\"].isnull().values.any()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d532a2",
   "metadata": {},
   "source": [
    "#### Creating lists of features dedicated to their data type to make it easy to search through them and find things like outliers. Excluding certain features as they are irrelevant and I do not want to mindlessly one-hot-encode them. I have excluded certain features in the features list as it increases my error when I one-hot-encode them. Therefore, I only have numeric features in my feature list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bde7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = [\"Year\", \"Capacity_cc\", \"Mileage_km\"]\n",
    "nominal_features = [\"Condition\", \"Transmission\", \"Fuel\"]\n",
    "features = [\"Year\", \"Capacity_cc\", \"Mileage_km\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e57f86",
   "metadata": {},
   "source": [
    "#### The values, in the case of numerical-valued features. Checking these values to see if there are any outliers. It is clear that there are many outliers here. For capacity, it is impossible for an engine to have a size of 1, 2, 3cc etc. These values need to be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e7166",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for feature in numeric_features:\n",
    "    print(feature, df1[feature].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749ddb9c",
   "metadata": {},
   "source": [
    "#### Illustrating how many rows and columns are in the dataframe before I start deleting outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb923fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6625740e",
   "metadata": {},
   "source": [
    "#### Checking to see what certain features look like on graphs and also checking to see where the outliers lie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd795fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = stripplot(x=\"Fuel\", y=\"Capacity_cc\", data=df1, jitter=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02337570",
   "metadata": {},
   "source": [
    "#### Delete examples whose Capacity is too small and too big. Selected these numbers from research on the web as to what the engine cc should be and by also looking at outliers on the graph\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a392f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = (df1[(df1[\"Capacity_cc\"] >= 300) & (df1[\"Capacity_cc\"] < 7000)]).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a22bcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = stripplot(x=\"Condition\", y=\"Price\", data=df1, jitter=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714d9410",
   "metadata": {},
   "source": [
    "#### Delete examples whose prices are too low and too high. 50000 value was decided through research into Sri Lankin car websites by checking the lowest price of a used car and seeing what the outliers were on the graphs. 158000000 was decided from finding out how much the most expensive car in sri lanka costs. No car exceeds this figure in Sri Lanka\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e59251",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = (df1[(df1[\"Price\"] > 50000) & (df1['Price'] <= 158000000)]).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fa4318",
   "metadata": {},
   "source": [
    "#### Illustrating that the examples have been deleted. If you look at the y-axis, it has narrowed down the range. This is why it may look like the \"New Condition\" chart has grown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3989d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = stripplot(x=\"Condition\", y=\"Price\", data=df1, jitter=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3aeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = stripplot(x=\"Condition\", y=\"Mileage_km\", data=df1, jitter=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25da619",
   "metadata": {},
   "source": [
    "#### Thinking of deleting used cars that have extremely low mileage but realised that there is no way of proving that they were not dirven\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd09675",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = (df1[(df1[\"Mileage_km\"] < 100) & (df1['Condition'] == 'Used')]).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc7754a",
   "metadata": {},
   "source": [
    "#### From my research, a cars life expectancy is 200,000 km. Taking engine replacemnents into account I have over doubled this figure to eliminate outliers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc63958b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df1 = (df1[(df1['Mileage_km'] < 500000)]).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f5920c",
   "metadata": {},
   "source": [
    "#### Reset the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6beaab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0816bd0a",
   "metadata": {},
   "source": [
    "#### Check shape after deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f379274",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820c7cd6",
   "metadata": {},
   "source": [
    "# Creating a Test Set\n",
    "#### Creating test set before data exploration so there is no leakage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d4f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df1, test_df1 = train_test_split(df1, train_size=0.8, random_state=2)\n",
    "\n",
    "# Extract the features but leave as a DataFrame\n",
    "dev_X = dev_df1[features]\n",
    "test_X = test_df1[features]\n",
    "\n",
    "# Target values, converted to a 1D numpy array\n",
    "dev_y = dev_df1[\"Price\"].values\n",
    "test_y = test_df1[\"Price\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1291135b",
   "metadata": {},
   "source": [
    "#### It can be good to do this on a copy of the dataset, excluding the test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b99068",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df1 = dev_df1.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3974f1",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce8c03b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = scatter_matrix(copy_df1, figsize=(15, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdb288a",
   "metadata": {},
   "source": [
    "#### Investigation into the price of vehicles. As expected, Manual cars price is low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dc088e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = stripplot(x=\"Transmission\", y=\"Price\", data=copy_df1, jitter=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7fee12",
   "metadata": {},
   "source": [
    "#### Attempting to do logs on graphs to see if any interesting changes are made. Attempting features like these for feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f67785",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = copy_df1.copy()\n",
    "df4['Capacity_cc'] = np.log(copy_df1['Capacity_cc'])\n",
    "plot = scatterplot(x=\"Capacity_cc\", y=\"Price\", data=df4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c537570",
   "metadata": {},
   "source": [
    "#### Illustrating the correlations between the numeric valued columns before feature engineering is performed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f36ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df1.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afc995c",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36539124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy_df1['ycmr'] = (copy_df1[\"Capacity_cc\"] + copy_df1[\"Year\"]) - copy_df1[\"Mileage_km\"]\n",
    "# copy_df1['ycr'] = copy_df1[\"Year\"] + np.log(copy_df1['Capacity_cc'])\n",
    "# copy_df1['ycr'] = copy_df1[\"Capacity_cc\"]\n",
    "copy_df1['ycr'] = (copy_df1[\"Capacity_cc\"] + copy_df1[\"Year\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19635998",
   "metadata": {},
   "source": [
    "#### Plotting my new feature against price to get a visualisation of the correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551b58b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = scatterplot(x=\"ycr\", y=\"Price\", data=copy_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d03be5",
   "metadata": {},
   "source": [
    "#### Checking the correlations between the standard features and the new features I have created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc96759",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy_df1.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8700555",
   "metadata": {},
   "source": [
    "#### Writing a class based on the new features created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a07d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InsertYCR(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, insert=True):\n",
    "        self.insert = insert\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self.insert:\n",
    "            X['ycr'] = X['Capacity_cc'] + X['Year']  \n",
    "            X = X.replace( [ np.inf, -np.inf ], np.nan )\n",
    "        return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb95a8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerFromHyperP(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, transformer=None):\n",
    "        self.transformer = transformer\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        if self.transformer:\n",
    "            self.transformer.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        if self.transformer:\n",
    "            return self.transformer.transform(X)\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5fffc0",
   "metadata": {},
   "source": [
    "# Preprocessor\n",
    "#### I discovered that one-hot-enoding suprisingly ended up increasing my prediction error so I left that out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53132ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "        (\"num\", Pipeline([(\"ycr\", InsertYCR()),\n",
    "                          (\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"mean\")),\n",
    "                          (\"scaler\", TransformerFromHyperP())]), \n",
    "                numeric_features)]\n",
    "    ,remainder=\"passthrough\")\n",
    "\n",
    "\n",
    "#         (\"nom\", Pipeline([(\"imputer\", SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\")), \n",
    "#                           (\"binarizer\", OneHotEncoder(handle_unknown=\"ignore\"))]), \n",
    "#                 nominal_features)],"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c017e153",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b81e83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline that combines the preprocessor with ridge regression\n",
    "ridge = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", Ridge())])\n",
    "\n",
    "# Create a dictionary of hyperparameters for ridge regression\n",
    "ridge_param_grid = {\"preprocessor__num__ycr__insert\": [True, False],\n",
    "                     \"preprocessor__num__scaler__transformer\": [StandardScaler(), MinMaxScaler(), RobustScaler()],\n",
    "                     \"predictor__alpha\": [80.0, 85.0, 90.0, 95.0, 100.0]}\n",
    "\n",
    "# Create the grid search object which will find the best hyperparameter values based on validation error\n",
    "ridge_gs = GridSearchCV(ridge, ridge_param_grid, scoring=\"neg_mean_absolute_error\", cv=10)\n",
    "\n",
    "# Run grid search by calling fit\n",
    "ridge_gs.fit(dev_X, dev_y)\n",
    "\n",
    "# Let's see how well we did\n",
    "ridge_gs.best_params_, ridge_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc1d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge.set_params(**ridge_gs.best_params_) \n",
    "scores = cross_validate(ridge, dev_X, dev_y, cv=10, \n",
    "                        scoring=\"neg_mean_absolute_error\", return_train_score=True)\n",
    "print(\"Training error: \", np.mean(np.abs(scores[\"train_score\"])))\n",
    "print(\"Validation error: \", np.mean(np.abs(scores[\"test_score\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3176ba",
   "metadata": {},
   "source": [
    "# Error Estimation\n",
    "#### As you can see, some models are much better than others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcb8e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4897bce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "quadratic_model = Pipeline([\n",
    "    (\"poly\", PolynomialFeatures(degree=3, include_bias=False)),\n",
    "    (\"predictor\", LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe1ab22",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model = Pipeline([\n",
    "    (\"poly\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "    (\"predictor\", LinearRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e146d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error estimation for the linear model.\n",
    "np.mean(cross_val_score(linear_model, dev_X, dev_y, scoring=\"neg_mean_absolute_error\", cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435a0611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error estimation for the quadratic model.\n",
    "np.mean(cross_val_score(quadratic_model, dev_X, dev_y, scoring=\"neg_mean_absolute_error\", cv=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73dca9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error estimation for the poly model.\n",
    "np.mean(cross_val_score(poly_model, dev_X, dev_y, scoring=\"neg_mean_absolute_error\", cv=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece91faa",
   "metadata": {},
   "source": [
    "# Under and Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f74f45",
   "metadata": {},
   "source": [
    "#### Underfitting. Note how high both kinds of error are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b14ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression()\n",
    "\n",
    "scores = cross_validate(linear_model, dev_X, dev_y, cv=10, scoring=\"neg_mean_absolute_error\", return_train_score=True)\n",
    "print(\"Training error: \", np.mean(np.abs(scores[\"train_score\"])))\n",
    "print(\"Validation error: \", np.mean(np.abs(scores[\"test_score\"])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce52065c",
   "metadata": {},
   "source": [
    "#### Overfitting. Polynomial regression with degree 10 overfits our synthetic training set. Note how low training error is, but validation error is high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736a9e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "poly_model_d10 = Pipeline([\n",
    "    (\"poly\", PolynomialFeatures(degree=10, include_bias=False)),\n",
    "    (\"predictor\", LinearRegression())\n",
    "])\n",
    "\n",
    "scores = cross_validate(poly_model_d10, dev_X, dev_y, cv=10, scoring=\"neg_mean_absolute_error\", return_train_score=True)\n",
    "print(\"Training error: \", np.mean(np.abs(scores[\"train_score\"])))\n",
    "print(\"Validation error: \", np.mean(np.abs(scores[\"test_score\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb1f2b",
   "metadata": {},
   "source": [
    "#### Just right. Quadratic model with degree 3 is about right for our synthetic training set. Note how training error is low but not surprisingly so, and validation error is similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b4304",
   "metadata": {},
   "outputs": [],
   "source": [
    "quartic_model = Pipeline([\n",
    "    (\"poly\", PolynomialFeatures(degree=3, include_bias=False)),\n",
    "    (\"predictor\", LinearRegression())\n",
    "])\n",
    "\n",
    "\n",
    "scores = cross_validate(quartic_model, dev_X, dev_y, cv=10, scoring=\"neg_mean_absolute_error\", return_train_score=True)\n",
    "print(\"Training error: \", np.mean(np.abs(scores[\"train_score\"])))\n",
    "print(\"Validation error: \", np.mean(np.abs(scores[\"test_score\"])))\n",
    "\n",
    "# quartic_model_gs.best_params_, quartic_model_gs.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a870dcdd",
   "metadata": {},
   "source": [
    "## Note\n",
    "#### When a model is OVERFITTING: it is too complex. You need to reduce the amount of features being used\n",
    "#### When a model is UNDERFITTING: it is not complex enough. You need to add more features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2918824f",
   "metadata": {},
   "source": [
    "# KNN\n",
    "#### This is the most accurate out of all of my prediction models, therefore this is the one I will use. Considering that the best nerest neighbour is not the last one we do not need to extend our range. If the best value was the last neighbour we would need to increase our range of neighbours to maybe 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29fed43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a pipeline that combines the preprocessor with kNN\n",
    "knn = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"predictor\", KNeighborsRegressor())])\n",
    "\n",
    "# Create a dictionary of hyperparameters for kNN\n",
    "knn_param_grid = {\"predictor__n_neighbors\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "                  \"preprocessor__num__ycr__insert\": [True, False],\n",
    "                  \"preprocessor__num__scaler__transformer\": [StandardScaler(), MinMaxScaler(), RobustScaler()]}\n",
    "\n",
    "# Create the grid search object which will find the best hyperparameter values based on validation error\n",
    "knn_gs = GridSearchCV(knn, knn_param_grid, scoring=\"neg_mean_absolute_error\", cv=10)\n",
    "\n",
    "# Run grid search by calling fit\n",
    "knn_gs.fit(dev_X, dev_y)\n",
    "\n",
    "# Let's see how well we did\n",
    "knn_gs.best_params_, knn_gs.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0e6fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_validate(knn, dev_X, dev_y, cv=10, scoring=\"neg_mean_absolute_error\", return_train_score=True)\n",
    "print(\"Training error: \", np.mean(np.abs(scores[\"train_score\"])))\n",
    "print(\"Validation error: \", np.mean(np.abs(scores[\"test_score\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72916a4c",
   "metadata": {},
   "source": [
    "# Evaluate on the test set\n",
    "#### Considering KNN has the lowest error for me on the training set, I will use this model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eb943e",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.set_params(**knn_gs.best_params_) \n",
    "knn.fit(dev_X, dev_y)\n",
    "mean_absolute_error(test_y, knn.predict(test_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79002a68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
